{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy-preserving Fake News Detection\n",
    "**Universidade de Bras√≠lia**<br>\n",
    "School of Technology<br>\n",
    "Graduate Program in Electrical Engineering (PPGEE)\n",
    "\n",
    "### Author: Stefano M P C Souza (stefanomozart@ieee.org)<br> Author: Daniel G Silva<br>Author: Anderson C A Nascimento\n",
    "\n",
    "# Privacy-preserving Model Training and Inference Setup\n",
    "\n",
    "Our general goal in this research work is to demonstrate how the use of secure Multi-party Computation (MPC) protocols can enable privacy-preserving fake news detection techniques. We are going to use neural networks inference models to classify news texts. The MPC protocols can be used both during the training and inference phases. \n",
    "\n",
    "In this notebook we setup the files that willl be used in each computing node. The generated folders and files will be used both for the ppml training and ppml inference experiments.\n",
    "\n",
    "**Notice:**\n",
    "In order to run this setup you need to first generate the training, validation and test subsets for each dataset. As weel as the embeddings used for text encoding. Refer to the [Classic NLP](./classic_nlp.ipynb) and the [BERT Based Embeddings](./embeddings.ipynb) notebooks for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:28.505152Z",
     "start_time": "2022-05-31T21:03:28.241626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os, sys, time, types, joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:28.837846Z",
     "start_time": "2022-05-31T21:03:28.507280Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.031280Z",
     "start_time": "2022-05-31T21:03:28.839080Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to your copy of CrypTen\n",
    "sys.path.insert(0, os.path.abspath('/home/ppml/CrypTen/'))\n",
    "import crypten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.035789Z",
     "start_time": "2022-05-31T21:03:29.032989Z"
    }
   },
   "outputs": [],
   "source": [
    "# - Computing parties\n",
    "ALICE = 0   # Will train the model\n",
    "BOB = 1     # Has the training and validation sets\n",
    "CHARLIE = 2 # Has the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.043423Z",
     "start_time": "2022-05-31T21:03:29.039869Z"
    }
   },
   "outputs": [],
   "source": [
    "# - Experiment globals\n",
    "args = types.SimpleNamespace()\n",
    "\n",
    "# List of datasets used in the experiments\n",
    "args.datasets = [\"liar\", \"sbnc\", \"fake.br\", \"factck.br\"]\n",
    "# Sentence_Transformer embbedings used to encode the texts\n",
    "args.embeddings = [\"stsb-distilbert-base\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
    "# Path to the NLP preprocessed datasets. Refer to the `classic_nlp.ipynb` \n",
    "# notebook for more details\n",
    "args.dataset_home = \"/home/ppml/datasets\"\n",
    "\n",
    "# Where to save the temp files\n",
    "args.output_path = './output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.058253Z",
     "start_time": "2022-05-31T21:03:29.045101Z"
    }
   },
   "outputs": [],
   "source": [
    "#- The Deep Neural Network models that will be trained\n",
    "\n",
    "# Path to our models\n",
    "sys.path.insert(0, sys.path.insert(0, os.path.abspath('../')))\n",
    "\n",
    "# Our Convolutional Neural Network\n",
    "from models.cnn import CNN, CNN2, CNN3, CNN4, CNN5\n",
    "\n",
    "# Our Deep Feed-Forward Neural Network\n",
    "from models.fnn import FNN, FNN2, FNN3, FNN4, FNN5\n",
    "\n",
    "args.models = [CNN, CNN2, CNN3, CNN4, CNN5, FNN, FNN2, FNN3, FNN4, FNN5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.100734Z",
     "start_time": "2022-05-31T21:03:29.059408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now that we have the environment set up, we init crypten\n",
    "crypten.init()\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.106036Z",
     "start_time": "2022-05-31T21:03:29.102271Z"
    }
   },
   "outputs": [],
   "source": [
    "#- Register dependencies\n",
    "crypten.common.serial.register_safe_class(types.SimpleNamespace)\n",
    "crypten.common.serial.register_safe_class(torch.nn.modules.activation.Tanh)\n",
    "crypten.common.serial.register_safe_class(torch.nn.modules.pooling.MaxPool1d)\n",
    "crypten.common.serial.register_safe_class(torch.nn.modules.dropout.Dropout)\n",
    "crypten.common.serial.register_safe_class(torch.nn.modules.pooling.MaxPool1d)\n",
    "crypten.common.serial.register_safe_class(torch.nn.modules.container.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.110981Z",
     "start_time": "2022-05-31T21:03:29.107431Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load a numpy array, convert to torch tensor and one-hot-encode labels\n",
    "eye = torch.eye(2)\n",
    "def load_torch(path, dtype=None):\n",
    "    arr = np.load(path, allow_pickle=True)\n",
    "    arr.setflags(write=True)\n",
    "    ten = torch.tensor(arr, dtype=dtype)\n",
    "    return eye[ten] if dtype==torch.long else ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.118740Z",
     "start_time": "2022-05-31T21:03:29.112268Z"
    }
   },
   "outputs": [],
   "source": [
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "# Run the preprocessing using CrypTen Multiprocess\n",
    "@mpc.run_multiprocess(world_size=3)\n",
    "def save_files(args):\n",
    "    # Identify the party running this code\n",
    "    rank = comm.get().get_rank()\n",
    "\n",
    "    # Alice will save a dummy/untrained copy of the models with her key\n",
    "    if args.experiment == 'ppml_training':\n",
    "        os.makedirs(f\"{args.output_path}/ppml_training/alice/plain\", exist_ok=True)\n",
    "\n",
    "        for model in args.models:\n",
    "            crypten.save_from_party(\n",
    "                model(),\n",
    "                f\"{args.output_path}/ppml_training/alice/plain/{model.__name__}.model\", \n",
    "                src=ALICE\n",
    "            )\n",
    "\n",
    "    # Bob and charlie will save the datasets\n",
    "    for d in args.datasets:\n",
    "        dtpath = f\"{args.dataset_home}/{d}\"\n",
    "\n",
    "        bobpath = f\"{args.output_path}/{args.experiment}/bob/{d}\"\n",
    "        os.makedirs(bobpath, exist_ok=True)\n",
    "        charliepath = f\"{args.output_path}/{args.experiment}/charlie/{d}\"\n",
    "        os.makedirs(charliepath, exist_ok=True)\n",
    "\n",
    "        # PPML Training setup\n",
    "        if args.experiment == 'ppml_training':\n",
    "            # Train labels\n",
    "            train_labels = load_torch(f\"{dtpath}/train.labels.npy\", dtype=torch.long)\n",
    "            crypten.save_from_party(train_labels, f\"{bobpath}/train.labels.ct\", src=BOB)\n",
    "\n",
    "            # Training ebeddings\n",
    "            for emb in args.embeddings:\n",
    "                train_embeddings = load_torch(f\"{dtpath}/train.{emb}.npy\")\n",
    "                crypten.save_from_party(train_embeddings, f\"{bobpath}/train.{emb}.ct\", src=BOB)\n",
    "\n",
    "            # Validation labels\n",
    "            valid_labels = load_torch(f\"{dtpath}/valid.labels.npy\", dtype=torch.long)\n",
    "            crypten.save_from_party(valid_labels, f\"{charliepath}/valid.labels.ct\", src=CHARLIE)\n",
    "\n",
    "            # Validation ebeddings\n",
    "            for emb in args.embeddings:\n",
    "                valid_embeddings = load_torch(f\"{dtpath}/valid.{emb}.npy\")\n",
    "                crypten.save_from_party(valid_embeddings, f\"{charliepath}/valid.{emb}.ct\", src=CHARLIE)\n",
    "        \n",
    "        # PPML Inference setup\n",
    "        if args.experiment == 'ppml_inference':\n",
    "            \n",
    "            # Test labels\n",
    "            test_labels = load_torch(f\"{dtpath}/test.labels.npy\", dtype=torch.long)\n",
    "            half = len(test_labels)//2\n",
    "            crypten.save_from_party(test_labels[:half,:], f\"{bobpath}/test.labels.ct\", src=BOB)\n",
    "            crypten.save_from_party(test_labels[half:,:], f\"{charliepath}/test.labels.ct\", src=CHARLIE)\n",
    "\n",
    "            # Test ebeddings\n",
    "            for emb in args.embeddings:\n",
    "                test_embeddings = load_torch(f\"{dtpath}/test.{emb}.npy\")\n",
    "                crypten.save_from_party(\n",
    "                    test_embeddings[:half,:], \n",
    "                    f\"{bobpath}/test.{emb}.ct\",\n",
    "                    src=BOB\n",
    "                )\n",
    "                crypten.save_from_party(\n",
    "                    test_embeddings[half:,:], \n",
    "                    f\"{charliepath}/test.{emb}.ct\",\n",
    "                    src=CHARLIE\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPML Training setup\n",
    "\n",
    "In this scenario, we are going to distribute the datasets and models according to our experiment desing: \n",
    "- Alice will have the untrained models;\n",
    "- Bob will have the training set; and\n",
    "- Charlie the validation set.\n",
    "\n",
    "Run the cell below to prepare the data to the privacy-preserving model training experiments. After running this code locally, or on alice's cloud node, send the folders under `OUTPUT_PATH/ppml_training` to the corresponding computing nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-31T21:03:29.405117Z",
     "start_time": "2022-05-31T21:03:29.119983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.experiment = 'ppml_training'\n",
    "\n",
    "save_files(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPML Inference setup\n",
    "\n",
    "In this scenario, we are going to distribute the datasets and models according to our experiment desing: \n",
    "- Alice will have the encrypted trained models;\n",
    "- Bob will have half of the test set; and\n",
    "- Charlie the other half.\n",
    "\n",
    "Run the cell below in order to prepare the data to the privacy-preserving inference experiments. After running this code on alice's cloud node, send the folders under `OUTPUT_PATH/ppml_inference` to the corresponding computing nodes.\n",
    "\n",
    "**Notice**: \n",
    "1. You nedd to run the PPML Training experiments before running the PPML Inference, in order to generate alices's trained, encrypted models\n",
    "2. Alternativelly, you can encrypt models trained on the clear and place them under the `OUTPUT_PATH/ppml_training/alice/encrypted` folder on alices' machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.experiment = 'ppml_inference'\n",
    "\n",
    "save_files(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

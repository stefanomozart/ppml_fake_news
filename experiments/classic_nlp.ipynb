{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection: an application of classic NLP techniques\n",
    "**Universidade de Brasília**<br>\n",
    "Faculdade de Tecnologia<br>\n",
    "Programa de Pós-graduação em Engenharia Elétrica (PPGEE)\n",
    "\n",
    "## Author: Stefano M P C Souza (stefanomozart@ieee.org)<br>Advisor: Daniel G Silva<br>Advisor: Anderson C A Nascimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Experiment design\n",
    "\n",
    "We want to study the impact of various NLP preprocessing techniques in the task of text classification for fake news detection. We are going to use the pipeline from [[1](#bot)] for model traing, tuning (hyper-parameter search) and comparison. The following ML algorithms are used:\n",
    "1. Naive Bayes:\n",
    "2. Decision Trees:\n",
    "2. K-Nearest Neighbour:\n",
    "3. Logistic Regression:\n",
    "3. Suport-Vector Machines:\n",
    "4. Random Forest:\n",
    "5. XGBoost:\n",
    "\n",
    "All models are trained and tested on a binary (*fake*/real) classification task. The *pipeline*, written by the author, extends the `sklearn.pipeline.Pipeline` class, from scikit-learn, and consists of the following steps:\n",
    "1. **Training and tuning**: uses a random search algorithm to select the best hyper-parameters for each ML model;\n",
    "2. **Selection**: for each dataset, selects the models with best performance, on the selected metric, for the validation set. The selected model is trained one more time with the concatanation of the training and the valiudation set;\n",
    "5. **Test**: the models selected on the previous step, and trained on training+validation sets are used to classify texts in the test set. The final score, on the selected metric, is record so we can compare ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Natural Language Processing\n",
    "\n",
    "### 2.1. Selected techniques\n",
    "\n",
    "1. **Tokenization**: the text, a sequence of caracters, is transformed in a ordered collection of tokens (words, punctiation marks, emojis, etc);\n",
    "2. **Stopword removal (SwR)**: removing words that do not add information, in the statistical learning sense, to any specific class in the sample. Most algorithms rely on experts dictionaries or on statistical measures such as *Mutual Information*;\n",
    "3. **Stemming**: Stemming is the reduction of variant forms of a word, eliminating inflectional morphemes such as verbal tense or plural suffixes, in order to provide a common representation, the root or stem. The intuition is to perform a dimensionality reduction on the dataset, removing rare morphological word variants, and reduce the risk of bias on word statistics measured on the documents;\n",
    "4. **Lemmatization:** Lemmatization consists on the reduction of each token to a linguistically valid root or lemma. The goal, from the statistical perspective, is exactly the same as in stemming: reduce variance in term frequency. It is sometimes compared to the normalization of the word sample, and aims to provide more accurate transformations than stemming, from the linguistic perspective;\n",
    "5. **Bag-of-Words (BoW)**: The BoW algorithm used in most NLP libraries is based on the *Vector Space Model* (VSM) and associates the tokens with with the corresponding term frequency: the number of occurrences of that token in that document. This algorithm produces an unordered set that does not retain any information on word order or proximity in the document ;\n",
    "6. **Term Frequency/Inverse Document Frequency (TF-IDF)**: Similar to the Vector Space Model Bag-of-Words, the TF-IDF (sometimes expressed as TF*IDF) document representation will associate each token in a document with a normalized or smoothed term frequency, weighted by the inverse of the frequency at which the term occurs in $D$, the corpus, or in the list of documents under processing. That is, $f_{t_i, d_j}$, the number of occurrences of token $t_i$ in document $d_j$, is replaced by $\\mathrm{tf\\cdot{idf}}$, where:\n",
    "   \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "  \\mathrm{tf}(t_i,d_j) &=1 + \\log \\frac{f_{t_i,d_j}}{\\sum_{t\\in d_j}{f_{t,d_j}}} \\\\\n",
    "  \\mathrm{idf}(t_i, D) &=  1 + \\log \\frac{|D|+1}{|\\{d \\in D : t_i \\in d\\}|+1}\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Datasets\n",
    "\n",
    "We selected 2 datasets in English and 2 in Portuguese. Each pair has a dataset with full-length news\n",
    "articles and a dataset comprised of short statements, or sentences. The purpose of experimenting\n",
    "with different languages and text sizes was to observe how these variables may impact preprocessing\n",
    "and training cost, and, ultimately, model performance.\n",
    "\n",
    "The selected datasets are:\n",
    "  - **Liar Dataset (liar):** curated by the UC Santa Barbara NLP Group, contains 12791 claims\n",
    "  by North-American politicians and celebrities, classified as `true`, `mostly-true`, `half-true`, \n",
    "  `barely-true`, `false` and `pants-on-fire` [[2](#liar)];\n",
    "\n",
    "  - **Source Based Fake News Classification (sbnc):** 2020 full-length news manually labeled\n",
    "  as `Real` or `Fake` [[3](#sbnc)];\n",
    "  \n",
    "  - **FactCk.br:** 1313 claims by Brazilian politicians, manually annotated by fact checking agencies\\footnote{\\url{https://piaui.folha.uol.com.br/lupa}, \\url{https://www.aosfatos.org} and \\url{https://apublica.org}} as `true`, `false`, `imprecise` and `others` [[4](#factckbr)];\n",
    "\n",
    "  - **Fake.br:** 7200 full-length news articles, with text and metadata, manually flagged as `real` or `fake` news [[5](#fakebr)].\n",
    "\n",
    "The classification experiments were preceded by a dataset preparation so that each dataset would have the same structure: \n",
    "1. **label**: (boolean) indicating if that text was labeled as *fake news*;\n",
    "2. **text**: (string) a concatenation of title (when available) and news body. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing\n",
    "#### Daset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-14T19:22:57.560218Z",
     "start_time": "2021-06-14T19:22:57.154409Z"
    }
   },
   "outputs": [],
   "source": [
    "# importando bibliotecas de propósito geral, utilizada na manipulação dos datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import os, sys, inspect, time\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T16:50:14.549039Z",
     "start_time": "2021-06-08T16:50:13.737019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Os datasets serão armezenados em um dicionário a fim de facilitar \n",
    "# a iteração de cada experimento sobre todos os datasets\n",
    "datasets = [\n",
    "    # Dataset 1: Liar \n",
    "    {'name':  'liar', 'lang': 'en', 'df': pd.read_csv('datasets/liar/liar.csv')},\n",
    "    \n",
    "    # Dataset 2: Source Based FK Detection\n",
    "    {'name': 'sbnc', 'lang': 'en', 'df': pd.read_csv('datasets/sbnc/sbnc.csv')},\n",
    "\n",
    "    # Dataset 3: Fake.br\n",
    "    {'name': 'fake.br', 'lang': 'pt', 'df': pd.read_csv('datasets/fake.br/fake.br.csv')},\n",
    "\n",
    "    # Dataset 4: FactCk.br\n",
    "    {'name': 'factck.br', 'lang': 'pt', 'df': pd.read_csv(\"datasets/factck.br/factck.br.csv\")}\n",
    "]\n",
    "\n",
    "experiments = {\n",
    "   \"E01\": {'preprocessing_time': {}, 'name': 'bow'},\n",
    "   \"E02\": {'preprocessing_time': {}, 'name': 'bow.swr'},\n",
    "   \"E03\": {'preprocessing_time': {}, 'name': 'bow.stem'},\n",
    "   \"E04\": {'preprocessing_time': {}, 'name': 'bow.lemm'},\n",
    "   \"E05\": {'preprocessing_time': {}, 'name': 'bow.lemm.swr'},\n",
    "   \"E06\": {'preprocessing_time': {}, 'name': 'tfidf'},\n",
    "   \"E07\": {'preprocessing_time': {}, 'name': 'tfidf.swr'},\n",
    "   \"E08\": {'preprocessing_time': {}, 'name': 'tfidf.stem'},\n",
    "   \"E09\": {'preprocessing_time': {}, 'name': 'tfidf.lemm'},\n",
    "   \"E10\": {'preprocessing_time': {}, 'name': 'tfidf.lemm.swr'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T16:50:20.268137Z",
     "start_time": "2021-06-08T16:50:16.856269Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for d in datasets:\n",
    "    train_valid, test = train_test_split(d['df'], stratify=d['df'].label, test_size=0.2, random_state=42)\n",
    "    train, valid = train_test_split(train_valid, stratify=train_valid.label, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_valid.to_csv(f\"datasets/{d['name']}/train.valid.csv\", index=False)\n",
    "    train.to_csv(f\"datasets/{d['name']}/train.csv\", index=False)\n",
    "    valid.to_csv(f\"datasets/{d['name']}/valid.csv\", index=False)\n",
    "    test.to_csv(f\"datasets/{d['name']}/test.csv\", index=False)\n",
    "    \n",
    "    d['train.valid'] = train_valid\n",
    "    d['train'] = train\n",
    "    d['valid'] = valid\n",
    "    d['test'] = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T16:51:28.675700Z",
     "start_time": "2021-06-08T16:51:09.509683Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    cv = CountVectorizer()    \n",
    "    train = cv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.bow.npz\", train)\n",
    "    valid = cv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.bow.npz\", valid)\n",
    "    \n",
    "    cv = CountVectorizer()\n",
    "    train = cv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.bow.npz\", train)\n",
    "    test = cv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.bow.npz\", test)\n",
    "    \n",
    "    experiments[\"E01\"]['preprocessing_time'][d['name']] = time.process_time() - t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. BoW and Stopword Removal  (BoW + SwR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T16:53:06.805855Z",
     "start_time": "2021-06-08T16:52:50.811802Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "swr = {\n",
    "    'en': nltk.corpus.stopwords.words(\"english\"), \n",
    "    'pt': nltk.corpus.stopwords.words(\"portuguese\")\n",
    "}\n",
    "\n",
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    cv = CountVectorizer(stop_words=swr[d['lang']])\n",
    "    train = cv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.bow.swr.npz\", train)\n",
    "    valid = cv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.bow.swr.npz\", valid)\n",
    "    \n",
    "    cv = CountVectorizer(stop_words=swr[d['lang']])\n",
    "    train = cv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.bow.swr.npz\", train)\n",
    "    test = cv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.bow.swr.npz\", test)\n",
    "    \n",
    "    experiments[\"E02\"]['preprocessing_time'][d['name']] = time.process_time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. BoW and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T16:53:06.816967Z",
     "start_time": "2021-06-08T16:53:06.807966Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "snowball = {\n",
    "    'en': nltk.stem.SnowballStemmer('english'),\n",
    "    'pt': nltk.stem.SnowballStemmer('portuguese')\n",
    "}\n",
    "\n",
    "def en_stemmer(doc):\n",
    "    return (snowball['en'].stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "def pt_stemmer(doc):\n",
    "    return (snowball['pt'].stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "cv_stemmer = {\n",
    "    'en': en_stemmer,\n",
    "    'pt': pt_stemmer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T16:57:04.306084Z",
     "start_time": "2021-06-08T16:53:07.428699Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    cv = CountVectorizer(analyzer=cv_stemmer[d['lang']])\n",
    "    train = cv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.bow.stem.npz\", train)\n",
    "    valid = cv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.bow.stem.npz\", valid)\n",
    "    \n",
    "    train = cv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.bow.stem.npz\", train)\n",
    "    test = cv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.bow.stem.npz\", test)\n",
    "    \n",
    "    experiments[\"E03\"]['preprocessing_time'][d['name']] = time.process_time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. BoW and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T16:57:11.363462Z",
     "start_time": "2021-06-08T16:57:04.308353Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 13:57:05 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| pos       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "=======================\n",
      "\n",
      "2021-06-08 13:57:05 INFO: Use device: gpu\n",
      "2021-06-08 13:57:05 INFO: Loading: tokenize\n",
      "2021-06-08 13:57:09 INFO: Loading: mwt\n",
      "2021-06-08 13:57:09 INFO: Loading: pos\n",
      "2021-06-08 13:57:11 INFO: Loading: lemma\n",
      "2021-06-08 13:57:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza_pt = stanza.Pipeline(lang='pt', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "wordnet = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def en_lemma(doc):\n",
    "    return [wordnet.lemmatize(token) for token in nltk.word_tokenize(doc)]\n",
    "    \n",
    "def pt_lemma(doc):\n",
    "    d = stanza_pt(doc).sentences\n",
    "    return [w.lemma for s in d for w in s.words]\n",
    "\n",
    "lemmatizer = {\n",
    "    'en': en_lemma,\n",
    "    'pt': pt_lemma\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T18:46:42.348325Z",
     "start_time": "2021-06-08T16:57:11.366015Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    cv = CountVectorizer(tokenizer=lemmatizer[d['lang']])\n",
    "    train = cv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.bow.lemm.npz\", train)\n",
    "    valid = cv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.bow.lemm.npz\", valid)\n",
    "    \n",
    "    cv = CountVectorizer(tokenizer=lemmatizer[d['lang']])\n",
    "    train = cv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.bow.lemm.npz\", train)\n",
    "    test = cv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.bow.lemm.npz\", test)\n",
    "    \n",
    "    experiments[\"E04\"]['preprocessing_time'][d['name']] = time.process_time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. BoW, Lemmatization and SwR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:10:07.689977Z",
     "start_time": "2021-06-08T18:46:42.350507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "/home/dev/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['estar', 'estivar', 'fôr', 'haver', 'ir', 'm', 'ser', 'ter', 'vós'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    cv = CountVectorizer(tokenizer=lemmatizer[d['lang']], stop_words=swr[d['lang']])    \n",
    "    train = cv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.bow.lemm.swr.npz\", train)\n",
    "    valid = cv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.bow.lemm.swr.npz\", valid)\n",
    "    \n",
    "    cv = CountVectorizer(tokenizer=lemmatizer[d['lang']], stop_words=swr[d['lang']])    \n",
    "    train = cv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.bow.lemm.swr.npz\", train)\n",
    "    test = cv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.bow.lemm.swr.npz\", test)\n",
    "    \n",
    "    experiments[\"E05\"]['preprocessing_time'][d['name']] = time.process_time() - t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Term-Frequency/Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:10:17.048211Z",
     "start_time": "2021-06-08T20:10:07.691493Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    tv = TfidfVectorizer()    \n",
    "    train = tv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.tfidf.npz\", train)    \n",
    "    valid = tv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.tfidf.npz\", valid)\n",
    "    \n",
    "    tv = TfidfVectorizer()\n",
    "    train = tv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.tfidf.npz\", train)\n",
    "    test = tv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.tfidf.npz\", test)    \n",
    "    \n",
    "    experiments[\"E06\"]['preprocessing_time'][d['name']] = time.process_time() - t\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. TF-IDF and SwR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:10:25.998571Z",
     "start_time": "2021-06-08T20:10:17.049398Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    tv = TfidfVectorizer(stop_words=swr[d['lang']])\n",
    "    train = tv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.tfidf.swr.npz\", train)\n",
    "    valid = tv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.tfidf.swr.npz\", valid)\n",
    "    \n",
    "    tv = TfidfVectorizer(stop_words=swr[d['lang']])\n",
    "    train = tv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.tfidf.swr.npz\", train)\n",
    "    test = tv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.tfidf.swr.npz\", test)\n",
    "    \n",
    "    experiments[\"E07\"]['preprocessing_time'][d['name']] = time.process_time() - t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. TF-IDF and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:10:26.004044Z",
     "start_time": "2021-06-08T20:10:26.000214Z"
    }
   },
   "outputs": [],
   "source": [
    "#norm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "tf_analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "snowball = {\n",
    "    'en': nltk.stem.SnowballStemmer('english'),\n",
    "    'pt': nltk.stem.SnowballStemmer('portuguese')\n",
    "}\n",
    "\n",
    "def en_stemmer(doc):\n",
    "    return (snowball['en'].stem(w) for w in tf_analyzer(doc))\n",
    "\n",
    "def pt_stemmer(doc):\n",
    "    return (snowball['pt'].stem(w) for w in tf_analyzer(doc))\n",
    "\n",
    "tf_stemmer = {\n",
    "    'en': en_stemmer,\n",
    "    'pt': pt_stemmer\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T20:12:40.069494Z",
     "start_time": "2021-06-08T20:10:26.005571Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    tv = TfidfVectorizer(tokenizer=tf_stemmer[d['lang']])\n",
    "    train = tv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.tfidf.stem.npz\", train)\n",
    "    valid = tv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.tfidf.stem.npz\", valid)\n",
    "    \n",
    "    tv = TfidfVectorizer(tokenizer=tf_stemmer[d['lang']])\n",
    "    train = tv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.tfidf.stem.npz\", train)\n",
    "    test = tv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.tfidf.stem.npz\", test)\n",
    "    \n",
    "    experiments[\"E08\"]['preprocessing_time'][d['name']] = time.process_time() - t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9. TF-IDF and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:38:37.636847Z",
     "start_time": "2021-06-08T20:12:40.070937Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    tv = TfidfVectorizer(tokenizer=lemmatizer[d['lang']])\n",
    "    train = tv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.tfidf.lemm.npz\", train)    \n",
    "    valid = tv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.tfidf.lemm.npz\", valid)\n",
    "    \n",
    "    tv = TfidfVectorizer(tokenizer=lemmatizer[d['lang']])\n",
    "    train = tv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.tfidf.lemm.npz\", train)    \n",
    "    test = tv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.tfidf.lemm.npz\", test)\n",
    "    \n",
    "    experiments[\"E09\"]['preprocessing_time'][d['name']] = time.process_time() - t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. TF-IDF, Lemmatization and SwR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T23:06:23.833498Z",
     "start_time": "2021-06-08T21:38:37.638239Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "for d in datasets:\n",
    "    t = time.process_time()\n",
    "    \n",
    "    tv = TfidfVectorizer(tokenizer=lemmatizer[d['lang']], stop_words=swr[d['lang']])\n",
    "    train = tv.fit_transform(d['train'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.tfidf.lemm.swr.npz\", train)\n",
    "    valid = tv.transform(d['valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/valid.tfidf.lemm.swr.npz\", valid)\n",
    "    \n",
    "    tv = TfidfVectorizer(tokenizer=lemmatizer[d['lang']], stop_words=swr[d['lang']])\n",
    "    train = tv.fit_transform(d['train.valid'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/train.valid.tfidf.lemm.swr.npz\", train)\n",
    "    test = tv.transform(d['test'].text)\n",
    "    save_npz(f\"datasets/{d['name']}/test.tfidf.lemm.swr.npz\", test)\n",
    "    \n",
    "    experiments[\"E10\"]['preprocessing_time'][d['name']] = time.process_time() - t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving the pre-processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T23:06:24.074495Z",
     "start_time": "2021-06-08T23:06:23.835292Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['experiments.pyd']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(datasets, 'datasets.pyd')\n",
    "joblib.dump(experiments, 'experiments.pyd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "<a name=\"bot\"></a>\n",
    "[1]: Souza, S.M.P. et al. *Tuning machine learning models to detect bots on Twitter*. 2020 Workshop on Communication Networks and Power Systems (WCNPS). Brasilia, 2020.\n",
    "\n",
    "<a name=\"liar\"></a>\n",
    "[2] Wlliam Yang Wang, \"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection, to appear in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), short paper, Vancouver, BC, Canada, July 30-August 4, ACL.\n",
    "\n",
    "<a name=\"snbc\"></a>\n",
    "[3]. A.  Bharadwaj,  B.  Ashar,  P.  Barbhaya,  R.  Bhatia,  Z.  Shaikh,  Source based fake news classification using machine learning (Aug 2020).URL https://kaggle.com/ruchi798/source-based-news-classification\n",
    "\n",
    "<a name=\"factbr\"></a>\n",
    "[4]. J. a. Moreno, G. Bressan, Factck.br:  A new dataset to study fake news,in:  Proceedings of the 25th Brazillian Symposium on Multimedia andthe  Web,  WebMedia  ’19,  Association  for  Computing  Machinery,  NewYork, NY, USA, 2019, p. 525–527.  doi:10.1145/3323503.3361698.\n",
    "\n",
    "<a name=\"fakebr\"></a>\n",
    "[5]. Monteiro R.A., Santos R.L.S., Pardo T.A.S., de Almeida T.A., Ruiz E.E.S., Vale O.A. (2018) Contributions to the Study of Fake News in Portuguese: New Corpus and Automatic Detection Results. In: Villavicencio A. et al. (eds) Computational Processing of the Portuguese Language. PROPOR 2018. Lecture Notes in Computer Science, vol 11122. Springer, Cham.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

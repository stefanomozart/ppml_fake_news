{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy-preserving Fake News Detection\n",
    "**Universidade de Bras√≠lia**<br>\n",
    "School of Technology<br>\n",
    "Graduate Program in Electrical Engineering (PPGEE)\n",
    "\n",
    "### Author: Stefano M P C Souza (stefanomozart@ieee.org)<br> Author: Daniel G Silva<br>Author: Anderson C A Nascimento\n",
    "\n",
    "# Clear-text Neural Network Benchmark\n",
    "\n",
    "Our general goal in this research work is to demonstrate the use of secure Multi-party Computation (MPC) protocols in order to provide privacy-preserving fake news detection techniques. We are going to use neural networks inference models to classify texts. The MPC protocols can be used both during the training and inference phases. \n",
    "\n",
    "In this notebook we train and test these neural networks in the clear-text setting --- that is, without any atempt cipher or keep the confidentiality of the datasets or models. We are going to use these results as a benchmark, in order to study the impact of running the same algorithms upon MPC protocols on the performance of the predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:20.589756Z",
     "start_time": "2022-02-28T18:56:20.334952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os, sys, time, random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:21.477255Z",
     "start_time": "2022-02-28T18:56:20.591607Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:21.970067Z",
     "start_time": "2022-02-28T18:56:21.478655Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:21.982726Z",
     "start_time": "2022-02-28T18:56:21.971265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to our models\n",
    "sys.path.insert(0, sys.path.insert(0, os.path.abspath('../')))\n",
    "\n",
    "# Our Convolution-LSTM Neural Network: consists of a conviolution followed by a LSTM\n",
    "from models.clstm import CLSTM\n",
    "\n",
    "# Our Recurrent Neural Network: consists in a LSTM followed by to dense layers\n",
    "from models.rnn import RNN\n",
    "\n",
    "# The CNN from [1]\n",
    "from models.adams import CNN\n",
    "\n",
    "models = [\n",
    "    {\n",
    "        'name': 'clstm',\n",
    "        'model': CLSTM\n",
    "    },\n",
    "    {\n",
    "        'name': 'rnn',\n",
    "        'model': RNN\n",
    "    },\n",
    "    {\n",
    "        'name': 'addams',\n",
    "        'model': CNN\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "The following functions are used to load the datasets and train the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:21.995977Z",
     "start_time": "2022-02-28T18:56:21.986916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split datasets in batches of this size  \n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# Use the CPU, since we are going to use only CPUs while running the MPC protocols \n",
    "# (A limitation we faced, because we could not get 4 GPU instances in any major cloud provider)\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:22.013914Z",
     "start_time": "2022-02-28T18:56:21.997861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model traing and test\n",
    "def train_test(model, model_name, optimizer, dataset, num_epochs, device, output_dir=\"\", save_model=True):\n",
    "    # Create output dir, if it does not exist\n",
    "    if save_model:\n",
    "        os.makedirs(output_dir + 'models', exist_ok=True)\n",
    "\n",
    "    best_loss = 1\n",
    "    \n",
    "    # Training\n",
    "    batch_index = [i for i in range(len(dataset['train']))]\n",
    "    t = time.process_time()\n",
    "    for epoch in range(num_epochs):\n",
    "        random.shuffle(batch_index)\n",
    "    \n",
    "        model.train()\n",
    "        losses = []\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # We are going to train the model with batches of BATCH_SIZE elements\n",
    "        for i in batch_index:\n",
    "            optimizer.zero_grad()\n",
    "            probs = model(dataset['train'][i])\n",
    "            loss = criterion(probs, dataset['train_label'][i])\n",
    "            losses.append(loss.item() / len(dataset['train'][i]))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss = np.mean(losses)\n",
    "        print(f\"Training loss on epoch {epoch}: {epoch_loss}\")\n",
    "        \n",
    "        info = evaluate(model, dataset['valid'], dataset['valid_label'])\n",
    "\n",
    "        if save_model and info['loss'] < best_loss:\n",
    "            torch.save(model, f\"{output_dir}{model_name}.best.model\")\n",
    "            torch.save(info, f\"{output_dir}{model_name}.best.info\")\n",
    "            \n",
    "            best_loss = info['loss']\n",
    "\n",
    "    train_runtime = time.process_time() - t\n",
    "    \n",
    "    if save_model:\n",
    "        torch.save(model, f\"{output_dir}{model_name}.last.model\")\n",
    "    \n",
    "    # Testing\n",
    "    t = time.process_time()\n",
    "    info = evaluate(model, dataset['test'], dataset['test_label'])\n",
    "    info['loss'] = losses\n",
    "    info['train_runtime'] = train_runtime \n",
    "    info['test_runtime'] = time.process_time() - t\n",
    "    \n",
    "    return info  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:22.026104Z",
     "start_time": "2022-02-28T18:56:22.015605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model evaluation: Cross-entropy losss, accuracy, F1-score and ROC AUC score\n",
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    y_predicted = []\n",
    "    y_true = []\n",
    "    \n",
    "    for bX, by in zip(X, y):\n",
    "        #embeddings, labels = tuple(i.to(device) for i in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = model(bX)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        loss = criterion(probs, by)\n",
    "        \n",
    "        losses.append(loss.item()/len(bX))\n",
    "        \n",
    "        predicted = probs.max(1).indices\n",
    "        \n",
    "        y_predicted.extend(predicted.tolist())\n",
    "        y_true.extend(by.tolist())\n",
    "    \n",
    "    return {\n",
    "        'loss': np.mean(losses), \n",
    "        'accuracy': accuracy_score(y_true, y_predicted), \n",
    "        'f1_score': f1_score(y_true, y_predicted),\n",
    "        'roc_auc': roc_auc_score(y_true, y_predicted)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:22.037516Z",
     "start_time": "2022-02-28T18:56:22.027535Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load a numpy array, convert to torch tensor and split in batches\n",
    "def load_torch_split(path, dtype=None, batch_size=BATCH_SIZE):\n",
    "    arr = np.load(path, allow_pickle=True)\n",
    "    ten = torch.tensor(arr, dtype=dtype)\n",
    "    return torch.split(ten, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments \n",
    "\n",
    "Check the [Embeddings](./embeddings.ipynb) notebook to see how the datasets were encoded to embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T18:56:22.065342Z",
     "start_time": "2022-02-28T18:56:22.039562Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experiments \n",
    "# - (check the Embeddings (embeddings.ipynb) notebook to see how the datasets were encoded to embeddings)\n",
    "embeddings = [\"stsb-distilbert-base\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
    "\n",
    "# Load dataset labels\n",
    "datasets = {}\n",
    "DATASET_HOME=\"/home/dev/datasets\"\n",
    "for d in [\"liar\", \"sbnc\", \"fake.br\", \"factck.br\"]:\n",
    "    dtpath = f'{DATASET_HOME}/{d}'\n",
    "    \n",
    "    datasets[d] = {\n",
    "        'name': d,\n",
    "        'train_label': load_torch_split(f\"{dtpath}/train.labels.npy\", dtype=torch.long),\n",
    "        'valid_label': load_torch_split(f\"{dtpath}/valid.labels.npy\", dtype=torch.long),\n",
    "        'test_label': load_torch_split(f\"{dtpath}/test.labels.npy\", dtype=torch.long)\n",
    "    }    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T19:00:58.606669Z",
     "start_time": "2022-02-28T18:56:22.066915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "liar stsb-distilbert-base clstm\n",
      "Training loss on epoch 0: 0.013427250717575126\n",
      "Training loss on epoch 1: 0.013086420212473188\n",
      "Training loss on epoch 2: 0.012881850386330477\n",
      "Training loss on epoch 3: 0.012653365045370542\n",
      "Training loss on epoch 4: 0.012330577087423113\n",
      "Training loss on epoch 5: 0.011763631327642382\n",
      "Training loss on epoch 6: 0.01101804677306152\n",
      "Training loss on epoch 7: 0.010106496402818568\n",
      "Training loss on epoch 8: 0.009203898626856687\n",
      "Training loss on epoch 9: 0.008292815872964545\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "liar stsb-distilbert-base rnn\n",
      "Training loss on epoch 0: 0.013524681331803989\n",
      "Training loss on epoch 1: 0.013077417391933216\n",
      "Training loss on epoch 2: 0.012856592937836664\n",
      "Training loss on epoch 3: 0.012583929917122844\n",
      "Training loss on epoch 4: 0.012129001305909107\n",
      "Training loss on epoch 5: 0.011426794965611931\n",
      "Training loss on epoch 6: 0.010403747395563623\n",
      "Training loss on epoch 7: 0.009482242552245536\n",
      "Training loss on epoch 8: 0.00828002075310991\n",
      "Training loss on epoch 9: 0.007206046891492835\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "liar stsb-distilbert-base addams\n",
      "Training loss on epoch 0: 0.013772735432672998\n",
      "Training loss on epoch 1: 0.01377766525496174\n",
      "Training loss on epoch 2: 0.01376401614955909\n",
      "Training loss on epoch 3: 0.013719733502806688\n",
      "Training loss on epoch 4: 0.01367413684050796\n",
      "Training loss on epoch 5: 0.013586575602820524\n",
      "Training loss on epoch 6: 0.013552349217054322\n",
      "Training loss on epoch 7: 0.013496999974242484\n",
      "Training loss on epoch 8: 0.013433663426583654\n",
      "Training loss on epoch 9: 0.013447462538393535\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "liar paraphrase-multilingual-mpnet-base-v2 clstm\n",
      "Training loss on epoch 0: 0.013340261329341851\n",
      "Training loss on epoch 1: 0.012984980921089026\n",
      "Training loss on epoch 2: 0.012882924453722062\n",
      "Training loss on epoch 3: 0.012810896674515062\n",
      "Training loss on epoch 4: 0.012759973728698304\n",
      "Training loss on epoch 5: 0.012708481537755775\n",
      "Training loss on epoch 6: 0.012714114156438087\n",
      "Training loss on epoch 7: 0.01261622866998566\n",
      "Training loss on epoch 8: 0.012398900516357156\n",
      "Training loss on epoch 9: 0.01196603166153622\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "liar paraphrase-multilingual-mpnet-base-v2 rnn\n",
      "Training loss on epoch 0: 0.013233784645899666\n",
      "Training loss on epoch 1: 0.012973384697262835\n",
      "Training loss on epoch 2: 0.012849798831270966\n",
      "Training loss on epoch 3: 0.012815783475541903\n",
      "Training loss on epoch 4: 0.012748394903405619\n",
      "Training loss on epoch 5: 0.012717516907208473\n",
      "Training loss on epoch 6: 0.012683114726161291\n",
      "Training loss on epoch 7: 0.012637905049614793\n",
      "Training loss on epoch 8: 0.012600706312610711\n",
      "Training loss on epoch 9: 0.012561975901342849\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "liar paraphrase-multilingual-mpnet-base-v2 addams\n",
      "Training loss on epoch 0: 0.01381519566118094\n",
      "Training loss on epoch 1: 0.01377109250228995\n",
      "Training loss on epoch 2: 0.0137619196402902\n",
      "Training loss on epoch 3: 0.01375822645342724\n",
      "Training loss on epoch 4: 0.013744633698297294\n",
      "Training loss on epoch 5: 0.01374298948755663\n",
      "Training loss on epoch 6: 0.013704890572443241\n",
      "Training loss on epoch 7: 0.013695544876703401\n",
      "Training loss on epoch 8: 0.013668039397495549\n",
      "Training loss on epoch 9: 0.013660179691239931\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "sbnc stsb-distilbert-base clstm\n",
      "Training loss on epoch 0: 0.013752885174838615\n",
      "Training loss on epoch 1: 0.0124430706343808\n",
      "Training loss on epoch 2: 0.011693661173621377\n",
      "Training loss on epoch 3: 0.011152306117199277\n",
      "Training loss on epoch 4: 0.010383231098398622\n",
      "Training loss on epoch 5: 0.009028832473597684\n",
      "Training loss on epoch 6: 0.0085546582145787\n",
      "Training loss on epoch 7: 0.007974169079091523\n",
      "Training loss on epoch 8: 0.006251792691020302\n",
      "Training loss on epoch 9: 0.006045426952860732\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "sbnc stsb-distilbert-base rnn\n",
      "Training loss on epoch 0: 0.013228791991432945\n",
      "Training loss on epoch 1: 0.012105299214740376\n",
      "Training loss on epoch 2: 0.01137426557449194\n",
      "Training loss on epoch 3: 0.011169536866984523\n",
      "Training loss on epoch 4: 0.009792501276884323\n",
      "Training loss on epoch 5: 0.008701365870652181\n",
      "Training loss on epoch 6: 0.0073949965043163994\n",
      "Training loss on epoch 7: 0.007316170625743412\n",
      "Training loss on epoch 8: 0.005958762645393937\n",
      "Training loss on epoch 9: 0.005605889786403259\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "sbnc stsb-distilbert-base addams\n",
      "Training loss on epoch 0: 0.013626337171474219\n",
      "Training loss on epoch 1: 0.013560212704288218\n",
      "Training loss on epoch 2: 0.013555558199410912\n",
      "Training loss on epoch 3: 0.013503439922035834\n",
      "Training loss on epoch 4: 0.013476403407998135\n",
      "Training loss on epoch 5: 0.013438391360171111\n",
      "Training loss on epoch 6: 0.013293800399853634\n",
      "Training loss on epoch 7: 0.013178060888807417\n",
      "Training loss on epoch 8: 0.012774519191120135\n",
      "Training loss on epoch 9: 0.012799865559780552\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "sbnc paraphrase-multilingual-mpnet-base-v2 clstm\n",
      "Training loss on epoch 0: 0.013287656558302293\n",
      "Training loss on epoch 1: 0.01175065827675355\n",
      "Training loss on epoch 2: 0.010999932755262424\n",
      "Training loss on epoch 3: 0.010449499076321015\n",
      "Training loss on epoch 4: 0.01012389518839099\n",
      "Training loss on epoch 5: 0.010120288521160573\n",
      "Training loss on epoch 6: 0.009865015366356887\n",
      "Training loss on epoch 7: 0.009067963528764118\n",
      "Training loss on epoch 8: 0.007552397087925956\n",
      "Training loss on epoch 9: 0.007121004469337917\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "sbnc paraphrase-multilingual-mpnet-base-v2 rnn\n",
      "Training loss on epoch 0: 0.012881012126639647\n",
      "Training loss on epoch 1: 0.01155076909851242\n",
      "Training loss on epoch 2: 0.010701063824442279\n",
      "Training loss on epoch 3: 0.010320956518143524\n",
      "Training loss on epoch 4: 0.010052829764701508\n",
      "Training loss on epoch 5: 0.009551775182559815\n",
      "Training loss on epoch 6: 0.009321830651480636\n",
      "Training loss on epoch 7: 0.008879852985942757\n",
      "Training loss on epoch 8: 0.007920926017311468\n",
      "Training loss on epoch 9: 0.007502652843356569\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "sbnc paraphrase-multilingual-mpnet-base-v2 addams\n",
      "Training loss on epoch 0: 0.013623491481983616\n",
      "Training loss on epoch 1: 0.013568626524327875\n",
      "Training loss on epoch 2: 0.013547581543415895\n",
      "Training loss on epoch 3: 0.013538183299176425\n",
      "Training loss on epoch 4: 0.013538396858470343\n",
      "Training loss on epoch 5: 0.013550560408896144\n",
      "Training loss on epoch 6: 0.013539653108233499\n",
      "Training loss on epoch 7: 0.013542181644247565\n",
      "Training loss on epoch 8: 0.01354765928490258\n",
      "Training loss on epoch 9: 0.013535471921002034\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "fake.br stsb-distilbert-base clstm\n",
      "Training loss on epoch 0: 0.013039537078590804\n",
      "Training loss on epoch 1: 0.011382120101041688\n",
      "Training loss on epoch 2: 0.01068465377374362\n",
      "Training loss on epoch 3: 0.009934574407275005\n",
      "Training loss on epoch 4: 0.009812269779623196\n",
      "Training loss on epoch 5: 0.009755190466680835\n",
      "Training loss on epoch 6: 0.009106362156009162\n",
      "Training loss on epoch 7: 0.009055748272647142\n",
      "Training loss on epoch 8: 0.008577828189378144\n",
      "Training loss on epoch 9: 0.008695480825119123\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "fake.br stsb-distilbert-base rnn\n",
      "Training loss on epoch 0: 0.01346434445791347\n",
      "Training loss on epoch 1: 0.011314008133385772\n",
      "Training loss on epoch 2: 0.010585503783277286\n",
      "Training loss on epoch 3: 0.010312846359065785\n",
      "Training loss on epoch 4: 0.00972766498884847\n",
      "Training loss on epoch 5: 0.009404630415862605\n",
      "Training loss on epoch 6: 0.009572315823326828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss on epoch 7: 0.009026928334146417\n",
      "Training loss on epoch 8: 0.008619134557503527\n",
      "Training loss on epoch 9: 0.008731904877449879\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "fake.br stsb-distilbert-base addams\n",
      "Training loss on epoch 0: 0.014701225222438894\n",
      "Training loss on epoch 1: 0.014642981845204548\n",
      "Training loss on epoch 2: 0.014596125137421392\n",
      "Training loss on epoch 3: 0.014023371922072543\n",
      "Training loss on epoch 4: 0.013369279942845782\n",
      "Training loss on epoch 5: 0.013150638393176501\n",
      "Training loss on epoch 6: 0.012639831035367904\n",
      "Training loss on epoch 7: 0.012571871774170988\n",
      "Training loss on epoch 8: 0.012412279584715443\n",
      "Training loss on epoch 9: 0.012463278401923437\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "fake.br paraphrase-multilingual-mpnet-base-v2 clstm\n",
      "Training loss on epoch 0: 0.010605422014831214\n",
      "Training loss on epoch 1: 0.009041427636659273\n",
      "Training loss on epoch 2: 0.0083641407718902\n",
      "Training loss on epoch 3: 0.008009088711552722\n",
      "Training loss on epoch 4: 0.007767069902150862\n",
      "Training loss on epoch 5: 0.007486339279560633\n",
      "Training loss on epoch 6: 0.007045224366248935\n",
      "Training loss on epoch 7: 0.006401323910041521\n",
      "Training loss on epoch 8: 0.006337034996719128\n",
      "Training loss on epoch 9: 0.005718212210803582\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "fake.br paraphrase-multilingual-mpnet-base-v2 rnn\n",
      "Training loss on epoch 0: 0.011072635644225665\n",
      "Training loss on epoch 1: 0.00903704934222724\n",
      "Training loss on epoch 2: 0.00830192565837855\n",
      "Training loss on epoch 3: 0.007824976231942895\n",
      "Training loss on epoch 4: 0.007500628156046713\n",
      "Training loss on epoch 5: 0.0070462060311148236\n",
      "Training loss on epoch 6: 0.00663177699090973\n",
      "Training loss on epoch 7: 0.006121494323816351\n",
      "Training loss on epoch 8: 0.0058841908747150045\n",
      "Training loss on epoch 9: 0.004959318266120009\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "fake.br paraphrase-multilingual-mpnet-base-v2 addams\n",
      "Training loss on epoch 0: 0.014671608658247097\n",
      "Training loss on epoch 1: 0.014644903819407185\n",
      "Training loss on epoch 2: 0.01442527449900104\n",
      "Training loss on epoch 3: 0.013977068710711695\n",
      "Training loss on epoch 4: 0.013870254415337761\n",
      "Training loss on epoch 5: 0.013733607559434828\n",
      "Training loss on epoch 6: 0.013569094655654763\n",
      "Training loss on epoch 7: 0.013672931912124797\n",
      "Training loss on epoch 8: 0.013540312426705513\n",
      "Training loss on epoch 9: 0.013542548539817975\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "factck.br stsb-distilbert-base clstm\n",
      "Training loss on epoch 0: 0.011150507120525135\n",
      "Training loss on epoch 1: 0.009306213277227738\n",
      "Training loss on epoch 2: 0.00861758400412167\n",
      "Training loss on epoch 3: 0.008307994718060773\n",
      "Training loss on epoch 4: 0.007944033119608373\n",
      "Training loss on epoch 5: 0.00743268674787353\n",
      "Training loss on epoch 6: 0.006609027745092617\n",
      "Training loss on epoch 7: 0.005386512616977973\n",
      "Training loss on epoch 8: 0.005637909739333041\n",
      "Training loss on epoch 9: 0.005247851509381743\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "factck.br stsb-distilbert-base rnn\n",
      "Training loss on epoch 0: 0.01089342187432682\n",
      "Training loss on epoch 1: 0.009075206193853828\n",
      "Training loss on epoch 2: 0.008538794613936368\n",
      "Training loss on epoch 3: 0.008324078575653189\n",
      "Training loss on epoch 4: 0.008757380755508647\n",
      "Training loss on epoch 5: 0.007776614024358636\n",
      "Training loss on epoch 6: 0.006410841591217939\n",
      "Training loss on epoch 7: 0.00616052933037281\n",
      "Training loss on epoch 8: 0.004148757457733155\n",
      "Training loss on epoch 9: 0.004743730495957768\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "factck.br stsb-distilbert-base addams\n",
      "Training loss on epoch 0: 0.01161245947375017\n",
      "Training loss on epoch 1: 0.010673568687018227\n",
      "Training loss on epoch 2: 0.010633686465375563\n",
      "Training loss on epoch 3: 0.010655536669142106\n",
      "Training loss on epoch 4: 0.010624328062814825\n",
      "Training loss on epoch 5: 0.010741316097624161\n",
      "Training loss on epoch 6: 0.010502829832189224\n",
      "Training loss on epoch 7: 0.010344312857179083\n",
      "Training loss on epoch 8: 0.010278493779547076\n",
      "Training loss on epoch 9: 0.010065063027774587\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "factck.br paraphrase-multilingual-mpnet-base-v2 clstm\n",
      "Training loss on epoch 0: 0.010332421011784497\n",
      "Training loss on epoch 1: 0.007861970119616564\n",
      "Training loss on epoch 2: 0.007049331489731283\n",
      "Training loss on epoch 3: 0.006633156117270974\n",
      "Training loss on epoch 4: 0.0061282034744234645\n",
      "Training loss on epoch 5: 0.006187138925580418\n",
      "Training loss on epoch 6: 0.005422104579560897\n",
      "Training loss on epoch 7: 0.004133642552530063\n",
      "Training loss on epoch 8: 0.004201872423291206\n",
      "Training loss on epoch 9: 0.0035514612189110587\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "factck.br paraphrase-multilingual-mpnet-base-v2 rnn\n",
      "Training loss on epoch 0: 0.009974663748460658\n",
      "Training loss on epoch 1: 0.007937470446614659\n",
      "Training loss on epoch 2: 0.007154037242426591\n",
      "Training loss on epoch 3: 0.006641960187869914\n",
      "Training loss on epoch 4: 0.00614994615316391\n",
      "Training loss on epoch 5: 0.005716739375801647\n",
      "Training loss on epoch 6: 0.005188298093921997\n",
      "Training loss on epoch 7: 0.004844608341946321\n",
      "Training loss on epoch 8: 0.004687976096482839\n",
      "Training loss on epoch 9: 0.003782584982759812\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "factck.br paraphrase-multilingual-mpnet-base-v2 addams\n",
      "Training loss on epoch 0: 0.01105805137578179\n",
      "Training loss on epoch 1: 0.010698015759972965\n",
      "Training loss on epoch 2: 0.0106926086194375\n",
      "Training loss on epoch 3: 0.010679877260152031\n",
      "Training loss on epoch 4: 0.010663444592672234\n",
      "Training loss on epoch 5: 0.010709554447847255\n",
      "Training loss on epoch 6: 0.010727396502214319\n",
      "Training loss on epoch 7: 0.010681390937636882\n",
      "Training loss on epoch 8: 0.010707534674335927\n",
      "Training loss on epoch 9: 0.010754138231277466\n"
     ]
    }
   ],
   "source": [
    "info = pd.DataFrame()\n",
    "for d in datasets.keys():\n",
    "    dtpath = f'{DATASET_HOME}/{d}'\n",
    "    for e in embeddings:\n",
    "        datasets[d]['train'] = load_torch_split(f\"{dtpath}/train.{e}.npy\")\n",
    "        datasets[d]['valid'] = load_torch_split(f\"{dtpath}/valid.{e}.npy\")\n",
    "        datasets[d]['test'] = load_torch_split(f\"{dtpath}/test.{e}.npy\")\n",
    "\n",
    "        output_path = f'./out/{d}/{e}/'\n",
    "        \n",
    "        for m in models:\n",
    "            print(\"\\n---------------------------------------------------------------------------------------\")\n",
    "            print(d, e, m['name'])\n",
    "\n",
    "            mdl = m['model']()\n",
    "            mdl_info = pd.DataFrame([\n",
    "                train_test(\n",
    "                    mdl,                               # Model instance\n",
    "                    m['name'],                         # Model name (to save)\n",
    "                    AdamW(mdl.parameters(), lr=0.002), # Optimizer\n",
    "                    datasets[d],                       # Dataset with train, valid and test sets\n",
    "                    10,                                # number of epochs for training\n",
    "                    DEVICE,                            # use GPU for torch operations, if available \n",
    "                    output_path                        # path to save trained models\n",
    "                )\n",
    "            ])\n",
    "            mdl_info['dataset'] = d\n",
    "            mdl_info['embedding'] = e\n",
    "            mdl_info['model'] = m['name']\n",
    "            \n",
    "            info = info.append(mdl_info, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T19:00:58.648771Z",
     "start_time": "2022-02-28T19:00:58.608218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>dataset</th>\n",
       "      <th>embedding</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[0.0076143044233322145, 0.0070717322826385496,...</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.719262</td>\n",
       "      <td>21.672421</td>\n",
       "      <td>0.111861</td>\n",
       "      <td>sbnc</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[0.004112387001514435, 0.007928777933120728, 0...</td>\n",
       "      <td>0.705446</td>\n",
       "      <td>0.745182</td>\n",
       "      <td>0.703432</td>\n",
       "      <td>17.487736</td>\n",
       "      <td>0.081008</td>\n",
       "      <td>sbnc</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0.0028282147645950317, 0.005094188451766968, ...</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.691240</td>\n",
       "      <td>17.522868</td>\n",
       "      <td>0.081122</td>\n",
       "      <td>sbnc</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[0.012314567565917969, 0.014219032526016235, 0...</td>\n",
       "      <td>0.650990</td>\n",
       "      <td>0.738404</td>\n",
       "      <td>0.607787</td>\n",
       "      <td>10.147545</td>\n",
       "      <td>0.080209</td>\n",
       "      <td>sbnc</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[0.007219417095184326, 0.0030452868342399596, ...</td>\n",
       "      <td>0.631188</td>\n",
       "      <td>0.618926</td>\n",
       "      <td>0.666701</td>\n",
       "      <td>23.612051</td>\n",
       "      <td>0.112323</td>\n",
       "      <td>sbnc</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[0.013074352741241455, 0.013525532484054565, 0...</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.196623</td>\n",
       "      <td>0.079061</td>\n",
       "      <td>sbnc</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.012171179056167603, 0.01265548586845398, 0....</td>\n",
       "      <td>0.618210</td>\n",
       "      <td>0.491411</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>128.535008</td>\n",
       "      <td>1.048806</td>\n",
       "      <td>liar</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.010766549110412598, 0.012709614038467407, 0...</td>\n",
       "      <td>0.614693</td>\n",
       "      <td>0.468177</td>\n",
       "      <td>0.590785</td>\n",
       "      <td>109.987746</td>\n",
       "      <td>0.513143</td>\n",
       "      <td>liar</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0112392520904541, 0.006361594200134277, 0.0...</td>\n",
       "      <td>0.599453</td>\n",
       "      <td>0.533455</td>\n",
       "      <td>0.590999</td>\n",
       "      <td>121.725314</td>\n",
       "      <td>0.660007</td>\n",
       "      <td>liar</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.008791337609291077, 0.007702667713165284, 0...</td>\n",
       "      <td>0.598671</td>\n",
       "      <td>0.514880</td>\n",
       "      <td>0.586555</td>\n",
       "      <td>109.002325</td>\n",
       "      <td>0.479136</td>\n",
       "      <td>liar</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.013056190013885497, 0.013492414951324463, 0...</td>\n",
       "      <td>0.564283</td>\n",
       "      <td>0.487356</td>\n",
       "      <td>0.554351</td>\n",
       "      <td>61.439082</td>\n",
       "      <td>0.449391</td>\n",
       "      <td>liar</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[0.013305749893188477, 0.012978882789611816, 0...</td>\n",
       "      <td>0.561938</td>\n",
       "      <td>0.058774</td>\n",
       "      <td>0.507050</td>\n",
       "      <td>60.061165</td>\n",
       "      <td>0.417739</td>\n",
       "      <td>liar</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[0.0041114529967308045, 0.006204996705055237, ...</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.829730</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>62.071632</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>fake.br</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[0.0037476581335067747, 0.005675026178359985, ...</td>\n",
       "      <td>0.817361</td>\n",
       "      <td>0.822417</td>\n",
       "      <td>0.817361</td>\n",
       "      <td>70.217404</td>\n",
       "      <td>0.352117</td>\n",
       "      <td>fake.br</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[0.007858449816703797, 0.00515763521194458, 0....</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.808424</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>61.306940</td>\n",
       "      <td>0.250914</td>\n",
       "      <td>fake.br</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[0.01061232089996338, 0.009570026993751526, 0....</td>\n",
       "      <td>0.800694</td>\n",
       "      <td>0.792480</td>\n",
       "      <td>0.800694</td>\n",
       "      <td>70.020527</td>\n",
       "      <td>0.370989</td>\n",
       "      <td>fake.br</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[0.010586888790130615, 0.01341386079788208, 0....</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.705584</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>35.392015</td>\n",
       "      <td>0.240195</td>\n",
       "      <td>fake.br</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[0.012333595752716064, 0.012592804431915284, 0...</td>\n",
       "      <td>0.619444</td>\n",
       "      <td>0.533220</td>\n",
       "      <td>0.619444</td>\n",
       "      <td>34.890440</td>\n",
       "      <td>0.239518</td>\n",
       "      <td>fake.br</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.005180470943450928, 0.004003062844276428, 0...</td>\n",
       "      <td>0.809886</td>\n",
       "      <td>0.882075</td>\n",
       "      <td>0.681954</td>\n",
       "      <td>17.075449</td>\n",
       "      <td>0.066210</td>\n",
       "      <td>factck.br</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.00264126718044281, 0.0020925332605838775, 0...</td>\n",
       "      <td>0.790875</td>\n",
       "      <td>0.872979</td>\n",
       "      <td>0.625405</td>\n",
       "      <td>11.450361</td>\n",
       "      <td>0.062089</td>\n",
       "      <td>factck.br</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.011681450605392456, 0.006447623968124389, 0...</td>\n",
       "      <td>0.783270</td>\n",
       "      <td>0.878465</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.754809</td>\n",
       "      <td>0.065554</td>\n",
       "      <td>factck.br</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.012361464500427246, 0.015047681331634522, 0...</td>\n",
       "      <td>0.783270</td>\n",
       "      <td>0.878465</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.446655</td>\n",
       "      <td>0.049208</td>\n",
       "      <td>factck.br</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>addams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.004750739932060242, 0.006298184394836426, 0...</td>\n",
       "      <td>0.779468</td>\n",
       "      <td>0.871111</td>\n",
       "      <td>0.554676</td>\n",
       "      <td>11.590060</td>\n",
       "      <td>0.062825</td>\n",
       "      <td>factck.br</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.004120964109897613, 0.0018853148818016052, ...</td>\n",
       "      <td>0.752852</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.664580</td>\n",
       "      <td>15.918054</td>\n",
       "      <td>0.067571</td>\n",
       "      <td>factck.br</td>\n",
       "      <td>stsb-distilbert-base</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 loss  accuracy  f1_score  \\\n",
       "9   [0.0076143044233322145, 0.0070717322826385496,...  0.712871  0.743363   \n",
       "10  [0.004112387001514435, 0.007928777933120728, 0...  0.705446  0.745182   \n",
       "7   [0.0028282147645950317, 0.005094188451766968, ...  0.688119  0.723684   \n",
       "8   [0.012314567565917969, 0.014219032526016235, 0...  0.650990  0.738404   \n",
       "6   [0.007219417095184326, 0.0030452868342399596, ...  0.631188  0.618926   \n",
       "11  [0.013074352741241455, 0.013525532484054565, 0...  0.603960  0.753086   \n",
       "3   [0.012171179056167603, 0.01265548586845398, 0....  0.618210  0.491411   \n",
       "4   [0.010766549110412598, 0.012709614038467407, 0...  0.614693  0.468177   \n",
       "0   [0.0112392520904541, 0.006361594200134277, 0.0...  0.599453  0.533455   \n",
       "1   [0.008791337609291077, 0.007702667713165284, 0...  0.598671  0.514880   \n",
       "2   [0.013056190013885497, 0.013492414951324463, 0...  0.564283  0.487356   \n",
       "5   [0.013305749893188477, 0.012978882789611816, 0...  0.561938  0.058774   \n",
       "16  [0.0041114529967308045, 0.006204996705055237, ...  0.825000  0.829730   \n",
       "15  [0.0037476581335067747, 0.005675026178359985, ...  0.817361  0.822417   \n",
       "13  [0.007858449816703797, 0.00515763521194458, 0....  0.804167  0.808424   \n",
       "12  [0.01061232089996338, 0.009570026993751526, 0....  0.800694  0.792480   \n",
       "14  [0.010586888790130615, 0.01341386079788208, 0....  0.677778  0.705584   \n",
       "17  [0.012333595752716064, 0.012592804431915284, 0...  0.619444  0.533220   \n",
       "21  [0.005180470943450928, 0.004003062844276428, 0...  0.809886  0.882075   \n",
       "22  [0.00264126718044281, 0.0020925332605838775, 0...  0.790875  0.872979   \n",
       "20  [0.011681450605392456, 0.006447623968124389, 0...  0.783270  0.878465   \n",
       "23  [0.012361464500427246, 0.015047681331634522, 0...  0.783270  0.878465   \n",
       "19  [0.004750739932060242, 0.006298184394836426, 0...  0.779468  0.871111   \n",
       "18  [0.004120964109897613, 0.0018853148818016052, ...  0.752852  0.838710   \n",
       "\n",
       "     roc_auc  train_runtime  test_runtime    dataset  \\\n",
       "9   0.719262      21.672421      0.111861       sbnc   \n",
       "10  0.703432      17.487736      0.081008       sbnc   \n",
       "7   0.691240      17.522868      0.081122       sbnc   \n",
       "8   0.607787      10.147545      0.080209       sbnc   \n",
       "6   0.666701      23.612051      0.112323       sbnc   \n",
       "11  0.500000      10.196623      0.079061       sbnc   \n",
       "3   0.597408     128.535008      1.048806       liar   \n",
       "4   0.590785     109.987746      0.513143       liar   \n",
       "0   0.590999     121.725314      0.660007       liar   \n",
       "1   0.586555     109.002325      0.479136       liar   \n",
       "2   0.554351      61.439082      0.449391       liar   \n",
       "5   0.507050      60.061165      0.417739       liar   \n",
       "16  0.825000      62.071632      0.272820    fake.br   \n",
       "15  0.817361      70.217404      0.352117    fake.br   \n",
       "13  0.804167      61.306940      0.250914    fake.br   \n",
       "12  0.800694      70.020527      0.370989    fake.br   \n",
       "14  0.677778      35.392015      0.240195    fake.br   \n",
       "17  0.619444      34.890440      0.239518    fake.br   \n",
       "21  0.681954      17.075449      0.066210  factck.br   \n",
       "22  0.625405      11.450361      0.062089  factck.br   \n",
       "20  0.500000       6.754809      0.065554  factck.br   \n",
       "23  0.500000       6.446655      0.049208  factck.br   \n",
       "19  0.554676      11.590060      0.062825  factck.br   \n",
       "18  0.664580      15.918054      0.067571  factck.br   \n",
       "\n",
       "                                embedding   model  \n",
       "9   paraphrase-multilingual-mpnet-base-v2   clstm  \n",
       "10  paraphrase-multilingual-mpnet-base-v2     rnn  \n",
       "7                    stsb-distilbert-base     rnn  \n",
       "8                    stsb-distilbert-base  addams  \n",
       "6                    stsb-distilbert-base   clstm  \n",
       "11  paraphrase-multilingual-mpnet-base-v2  addams  \n",
       "3   paraphrase-multilingual-mpnet-base-v2   clstm  \n",
       "4   paraphrase-multilingual-mpnet-base-v2     rnn  \n",
       "0                    stsb-distilbert-base   clstm  \n",
       "1                    stsb-distilbert-base     rnn  \n",
       "2                    stsb-distilbert-base  addams  \n",
       "5   paraphrase-multilingual-mpnet-base-v2  addams  \n",
       "16  paraphrase-multilingual-mpnet-base-v2     rnn  \n",
       "15  paraphrase-multilingual-mpnet-base-v2   clstm  \n",
       "13                   stsb-distilbert-base     rnn  \n",
       "12                   stsb-distilbert-base   clstm  \n",
       "14                   stsb-distilbert-base  addams  \n",
       "17  paraphrase-multilingual-mpnet-base-v2  addams  \n",
       "21  paraphrase-multilingual-mpnet-base-v2   clstm  \n",
       "22  paraphrase-multilingual-mpnet-base-v2     rnn  \n",
       "20                   stsb-distilbert-base  addams  \n",
       "23  paraphrase-multilingual-mpnet-base-v2  addams  \n",
       "19                   stsb-distilbert-base     rnn  \n",
       "18                   stsb-distilbert-base   clstm  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.sort_values(by=['dataset', 'accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model, as measured by accuracy on the test set, for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T19:00:58.666283Z",
     "start_time": "2022-02-28T19:00:58.650809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>embedding</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>factck.br</th>\n",
       "      <td>[0.005180470943450928, 0.004003062844276428, 0...</td>\n",
       "      <td>0.809886</td>\n",
       "      <td>0.882075</td>\n",
       "      <td>0.681954</td>\n",
       "      <td>17.075449</td>\n",
       "      <td>0.066210</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake.br</th>\n",
       "      <td>[0.0041114529967308045, 0.006204996705055237, ...</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.829730</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>62.071632</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liar</th>\n",
       "      <td>[0.012171179056167603, 0.01265548586845398, 0....</td>\n",
       "      <td>0.618210</td>\n",
       "      <td>0.491411</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>128.535008</td>\n",
       "      <td>1.048806</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sbnc</th>\n",
       "      <td>[0.0076143044233322145, 0.0070717322826385496,...</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.719262</td>\n",
       "      <td>21.672421</td>\n",
       "      <td>0.111861</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        loss  accuracy  \\\n",
       "dataset                                                                  \n",
       "factck.br  [0.005180470943450928, 0.004003062844276428, 0...  0.809886   \n",
       "fake.br    [0.0041114529967308045, 0.006204996705055237, ...  0.825000   \n",
       "liar       [0.012171179056167603, 0.01265548586845398, 0....  0.618210   \n",
       "sbnc       [0.0076143044233322145, 0.0070717322826385496,...  0.712871   \n",
       "\n",
       "           f1_score   roc_auc  train_runtime  test_runtime  \\\n",
       "dataset                                                      \n",
       "factck.br  0.882075  0.681954      17.075449      0.066210   \n",
       "fake.br    0.829730  0.825000      62.071632      0.272820   \n",
       "liar       0.491411  0.597408     128.535008      1.048806   \n",
       "sbnc       0.743363  0.719262      21.672421      0.111861   \n",
       "\n",
       "                                       embedding  model  \n",
       "dataset                                                  \n",
       "factck.br  paraphrase-multilingual-mpnet-base-v2  clstm  \n",
       "fake.br    paraphrase-multilingual-mpnet-base-v2    rnn  \n",
       "liar       paraphrase-multilingual-mpnet-base-v2  clstm  \n",
       "sbnc       paraphrase-multilingual-mpnet-base-v2  clstm  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.sort_values(by=['accuracy'], ascending=False).groupby(by='dataset').first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model, as measured by the ROC AUC metric on the test set, for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T19:05:16.488658Z",
     "start_time": "2022-02-28T19:05:16.470613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>test_runtime</th>\n",
       "      <th>embedding</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>factck.br</th>\n",
       "      <td>[0.005180470943450928, 0.004003062844276428, 0...</td>\n",
       "      <td>0.809886</td>\n",
       "      <td>0.882075</td>\n",
       "      <td>0.681954</td>\n",
       "      <td>17.075449</td>\n",
       "      <td>0.066210</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fake.br</th>\n",
       "      <td>[0.0041114529967308045, 0.006204996705055237, ...</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.829730</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>62.071632</td>\n",
       "      <td>0.272820</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>rnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liar</th>\n",
       "      <td>[0.012171179056167603, 0.01265548586845398, 0....</td>\n",
       "      <td>0.618210</td>\n",
       "      <td>0.491411</td>\n",
       "      <td>0.597408</td>\n",
       "      <td>128.535008</td>\n",
       "      <td>1.048806</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sbnc</th>\n",
       "      <td>[0.0076143044233322145, 0.0070717322826385496,...</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.719262</td>\n",
       "      <td>21.672421</td>\n",
       "      <td>0.111861</td>\n",
       "      <td>paraphrase-multilingual-mpnet-base-v2</td>\n",
       "      <td>clstm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        loss  accuracy  \\\n",
       "dataset                                                                  \n",
       "factck.br  [0.005180470943450928, 0.004003062844276428, 0...  0.809886   \n",
       "fake.br    [0.0041114529967308045, 0.006204996705055237, ...  0.825000   \n",
       "liar       [0.012171179056167603, 0.01265548586845398, 0....  0.618210   \n",
       "sbnc       [0.0076143044233322145, 0.0070717322826385496,...  0.712871   \n",
       "\n",
       "           f1_score   roc_auc  train_runtime  test_runtime  \\\n",
       "dataset                                                      \n",
       "factck.br  0.882075  0.681954      17.075449      0.066210   \n",
       "fake.br    0.829730  0.825000      62.071632      0.272820   \n",
       "liar       0.491411  0.597408     128.535008      1.048806   \n",
       "sbnc       0.743363  0.719262      21.672421      0.111861   \n",
       "\n",
       "                                       embedding  model  \n",
       "dataset                                                  \n",
       "factck.br  paraphrase-multilingual-mpnet-base-v2  clstm  \n",
       "fake.br    paraphrase-multilingual-mpnet-base-v2    rnn  \n",
       "liar       paraphrase-multilingual-mpnet-base-v2  clstm  \n",
       "sbnc       paraphrase-multilingual-mpnet-base-v2  clstm  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.sort_values(by=['roc_auc'], ascending=False).groupby(by='dataset').first()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
